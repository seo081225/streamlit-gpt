import streamlit as st
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import UnstructuredFileLoader, PyPDFLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.memory import ConversationBufferMemory
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.storage import LocalFileStore
from langchain.prompts import ChatPromptTemplate
import tempfile
import os
import requests

def is_valid_api_key(api_key):
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    try:
        response = requests.get(
            "https://api.openai.com/v1/models", headers=headers)
        return response.status_code == 200
    except requests.RequestException:
        return False

st.title("ğŸ“„ Document GPT")
st.markdown(
    """
## Welcome!

ì±—ë´‡ì„ ì‚¬ìš©í•˜ì—¬ AIì—ê²Œ íŒŒì¼ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”!

openAPI ì‚¬ìš©ì„ ìœ„í•´ sidebarì— API Keyë¥¼ ì…ë ¥í›„ ë¬¸ì„œë¥¼ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.

API í‚¤ëŠ” ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
"""
)

cache_dir = LocalFileStore("./.cache/")
uploaded_file = ""

with st.sidebar:
    openai_api_key = st.text_input("OpenAI API Key", type="password")
    if not openai_api_key:
        st.info("OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", icon="ğŸ”‘")
    if openai_api_key:
        if is_valid_api_key(openai_api_key):
            os.environ["OPENAI_API_KEY"] = openai_api_key
            st.success("API í‚¤ê°€ ìœ íš¨í•©ë‹ˆë‹¤.")
            uploaded_file = st.file_uploader(
        "ë¬¸ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš” (.txt, .pdf, .docx)", type=("txt", "pdf", "docx")
    )
        else:
            st.warning("ìœ íš¨í•œ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")



question = st.text_area(
    "ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸ì„ í•˜ì„¸ìš”",
    placeholder="ë¬¸ì„œ ìš”ì•½ì¢€ í•´ì¤„ë˜?",
    disabled=not uploaded_file,
)

if openai_api_key and uploaded_file and question:
    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
        tmp_file.write(uploaded_file.read())
        tmp_file_path = tmp_file.name

    llm = ChatOpenAI(api_key=openai_api_key, model="gpt-4o-mini", temperature=0.1)

    # íŒŒì¼ í˜•ì‹ì— ë§ëŠ” ë¡œë” ì„ íƒ
    file_extension = os.path.splitext(uploaded_file.name)[1].lower()

    if file_extension == ".pdf":
        loader = PyPDFLoader(tmp_file_path)  # PDF íŒŒì¼ ì²˜ë¦¬
    elif file_extension == ".txt":
        loader = TextLoader(tmp_file_path)  # í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬
    elif file_extension == ".docx":
        loader = UnstructuredFileLoader(tmp_file_path)  # DOCX íŒŒì¼ ì²˜ë¦¬
    else:
        st.error("ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
        st.stop()

    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=600,
        chunk_overlap=100,
    )

    docs = loader.load_and_split(text_splitter=splitter)

    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vectorstore = FAISS.from_documents(docs, embeddings)
    retriever = vectorstore.as_retriever()

    prompt_template = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ì½ê³  í•´ì„í•©ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ìƒì„¸í•˜ê²Œ ëŒ€ë‹µí•˜ì‹­ì‹œì˜¤. ë‹µì„ ëª¨ë¥´ë©´ ì§€ì–´ë‚´ì§€ ë§ê³  ëª¨ë¥¸ë‹¤ê³  ëŒ€ë‹µí•˜ì‹­ì‹œì˜¤.\n\n{context}",
            ),
            ("human", "{question}"),
        ]
    )

    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    def chain_with_memory(question):
        chat_history = memory.load_memory_variables({}).get("chat_history", "")

        relevant_docs = retriever.get_relevant_documents(question)
        context = "\n\n".join([doc.page_content for doc in relevant_docs])

        inputs = {
            "context": context,
            "question": question,
            "chat_history": chat_history,
        }

        prompt = prompt_template.format_messages(**inputs)
        response = llm(prompt)

        memory.save_context({"question": question}, {"answer": response.content})

        return response.content

    if st.button("ì§ˆë¬¸ì— ë‹µë³€ ë°›ê¸°"):
        with st.spinner("GPTê°€ ë¬¸ì„œë¥¼ ì½ê³  ë‹µë³€ ì¤‘ì…ë‹ˆë‹¤..."):
            answer = chain_with_memory(question)
            st.write("### ğŸ¤–")
            st.write(answer)

# ì˜¤ë¥˜ ë˜ëŠ” ì•Œë¦¼ ì²˜ë¦¬
elif not openai_api_key:
    st.info("OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
elif not is_valid_api_key(openai_api_key):
    st.warning("ìœ íš¨í•œ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
elif not uploaded_file:
    st.info("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
elif not question:
    st.info("ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
